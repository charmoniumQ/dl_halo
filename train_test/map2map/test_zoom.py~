import sys
from pprint import pprint
import numpy as np
import torch
from torch.utils.data import DataLoader, Subset

from .data import FieldDataset, FieldDataset_zoom
from . import models
from .models import narrow_cast
from .utils import import_attr, load_model_state_dict

import time
from datetime import datetime
from torch.multiprocessing import spawn
import os


def node_worker(args):
    if 'SLURM_STEP_NUM_NODES' in os.environ:
        args.nodes = int(os.environ['SLURM_STEP_NUM_NODES'])
    elif 'SLURM_JOB_NUM_NODES' in os.environ:
        args.nodes = int(os.environ['SLURM_JOB_NUM_NODES'])
    else:
        raise KeyError('missing node counts in slurm env')
        
    #args.gpus_per_node = torch.get_num_threads()  #didnt change to cpus_per_node out of lazyness
    #args.gpus_per_node = int(os.environ['SLURM_CPUS_PER_TASK'])
    args.gpus_per_node = 10;
    args.world_size = args.nodes * args.gpus_per_node

    node = int(os.environ['SLURM_NODEID'])
    
    run_name = args.train_run_name #name of training run that's loaded
    
    #make dir for saving the epoch states
    if not os.path.exists('test'):
        os.makedirs('test')
    now = datetime.now()
    save_to = run_name + '_' + str(os.environ['SLURM_JOB_NAME']) + '_' + str(os.environ['SLURM_JOB_NODELIST'])
    
    #save_to = '2020_September_13--14_21_0__train_zoom_cgan_loooooow_further_gpu-54_test_newUNet_zoom_c21-02,c24-03' #set manually
    
    if node != 0: time.sleep(5) #wait a second for node 0 to create folder
    if not os.path.exists('./test/'+save_to):
        os.makedirs('./test/'+save_to)
    #if not os.path.exists('./test/'+args.cube_name):
    #    os.makedirs('./test/'+args.cube_name)
        
    #print for slurm output:
    print('Run Name: ', run_name)
    print('save_to:', save_to)
    print()
    
    pprint(vars(args))
    sys.stdout.flush()

    spawn(cpu_worker, args=(node, args, save_to), nprocs=args.gpus_per_node)


def cpu_worker(local_rank, node, args, save_to):

    device = torch.device('cpu')

    torch.set_num_threads(2)

    rank = args.gpus_per_node * node + local_rank

    test_dataset = FieldDataset_zoom(
        in_patterns=args.test_in_patterns,
        tgt_patterns=args.test_tgt_patterns,
        in_norms=args.in_norms,
        tgt_norms=args.tgt_norms,
        callback_at=args.callback_at,
        augment=False,
        aug_shift=None,
        aug_add=None,
        aug_mul=None,
        crop=args.crop,
        crop_start=args.crop_start,
        crop_stop=args.crop_stop,
        crop_step=args.crop_step,
        pad=args.pad,
        scale_factor=args.scale_factor,
    )
    
    n_data = len(test_dataset)
    rank_indices = np.array_split(np.arange(n_data), args.world_size)[rank].tolist()
    smaller_test_set = Subset(test_dataset, rank_indices)
    
    test_loader = DataLoader(
        smaller_test_set,
        batch_size=args.batches,
        shuffle=False,
        num_workers=args.loader_workers,
    )

    in_chan, out_chan = test_dataset.in_chan, test_dataset.tgt_chan

    model = import_attr(args.model, models, callback_at=args.callback_at)
    model = model(sum(in_chan), sum(out_chan)) #, scale_factor=args.scale_factor
    try:
        criterion = import_attr(args.criterion, torch.nn, callback_at=args.callback_at)
    except:
        criterion = getattr(losses, args.criterion)
    criterion = criterion()

    device = torch.device('cpu')
    state = torch.load(args.load_state, map_location=device)
    load_model_state_dict(model, state['model'], strict=args.load_state_strict)
    #print('model state at epoch {} loaded from {}'.format(
    #    state['epoch'], args.load_state))
    del state

    model.eval()
    
    
    """#create list of already existing testing cubes in that testing dir (when the testing times out and you want to continue). Be sure that you also use the same save_to path (set it manually here in code)
    run_path = './test/'+save_to
    
    list_cubes_with_paths = []
    list_subfolders_with_paths_0 = [f.path for f in os.scandir(run_path) if f.is_dir()]
    for direc in list_subfolders_with_paths_0:
        list_subfolders_with_paths_1 = [f.path for f in os.scandir(direc) if f.is_dir()]
        for direc2 in list_subfolders_with_paths_1:
            list_cubes_with_paths.extend([f.path for f in os.scandir(direc2) if f.is_file()])
    #remove path from cubenames for later checking:
    f = []
    for el in list_cubes_with_paths:
        cubepath = el[:-4] #remove .npy from name
        name = cubepath.split('/')[-1]
        f.append(name)
    del list_cubes_with_paths"""

    with torch.no_grad():
        for i, (input, target, in_fpath, tgt_fpath) in enumerate(test_loader):
            #get filename of test_cube (batchsize == 1)
            sample_fname_tot = in_fpath[0][0].split('/')[-1] #get name of 1st in_field
            sample_fname = sample_fname_tot[:-4] #remove .npy ending
            
            """#check if cube already exists, if yes: skip testing cube
            if sample_fname_tot in np.array(f):
                continue"""
            
            
            output = model(input)
            input, output, target = narrow_cast(input, output, target)

            loss = criterion(output, target)

            
            
            #undo norms from data before saving
            if args.in_norms is not None:
                start = 0
                for norm, stop in zip(test_dataset.in_norms, np.cumsum(in_chan)):
                    norm(input[:, start:stop], undo=True)
                    start = stop
            if args.tgt_norms is not None:
                start = 0
                for norm, stop in zip(test_dataset.tgt_norms, np.cumsum(out_chan)):
                    norm(output[:, start:stop], undo=True)
                    norm(target[:, start:stop], undo=True)
                    start = stop

            
            
            def subfolderSave(pos, frac, lenloader, path):
                for j in range(10):
                    if pos < frac*lenloader + lenloader/100*(j+1):
                        if not os.path.exists(path+str(j)+'/'):
                            os.makedirs(path+str(j)+'/')
                        np.savez(path+str(j)+'/{}.npz'.format(sample_fname), input=input.numpy(),
                                    output=output.numpy(), target=target.numpy())
                        break
            #save cube to 0-9 subfolder
            
            if i < len(test_loader)*0.1:
                pathh = './test/'+save_to+'/0/'
                if not os.path.exists(pathh):
                    os.makedirs(pathh)
                subfolderSave(i, 0.0, len(test_loader), pathh)
            elif i < len(test_loader)*0.2:
                pathh = './test/'+save_to+'/1/'
                if not os.path.exists(pathh):
                    os.makedirs(pathh)
                subfolderSave(i, 0.1, len(test_loader), pathh)
            elif i < len(test_loader)*0.3:
                pathh = './test/'+save_to+'/2/'
                if not os.path.exists(pathh):
                    os.makedirs(pathh)
                subfolderSave(i, 0.2, len(test_loader), pathh)
            elif i < len(test_loader)*0.4:
                pathh = './test/'+save_to+'/3/'
                if not os.path.exists(pathh):
                    os.makedirs(pathh)
                subfolderSave(i, 0.3, len(test_loader), pathh)
            elif i < len(test_loader)*0.5:
                pathh = './test/'+save_to+'/4/'
                if not os.path.exists(pathh):
                    os.makedirs(pathh)
                subfolderSave(i, 0.4, len(test_loader), pathh)
            elif i < len(test_loader)*0.6:
                pathh = './test/'+save_to+'/5/'
                if not os.path.exists(pathh):
                    os.makedirs(pathh)
                subfolderSave(i, 0.5, len(test_loader), pathh)
            elif i < len(test_loader)*0.7:
                pathh = './test/'+save_to+'/6/'
                if not os.path.exists(pathh):
                    os.makedirs(pathh)
                subfolderSave(i, 0.6, len(test_loader), pathh)
            elif i < len(test_loader)*0.8:
                pathh = './test/'+save_to+'/7/'
                if not os.path.exists(pathh):
                    os.makedirs(pathh)
                subfolderSave(i, 0.7, len(test_loader), pathh)
            elif i < len(test_loader)*0.9:
                pathh = './test/'+save_to+'/8/'
                if not os.path.exists(pathh):
                    os.makedirs(pathh)
                subfolderSave(i, 0.8, len(test_loader), pathh)
            else:
                pathh = './test/'+save_to+'/9/'
                if not os.path.exists(pathh):
                    os.makedirs(pathh)
                subfolderSave(i, 0.9, len(test_loader), pathh)

            #np.savez('./test/'+save_to+'/{}.npz'.format(sample_fname), input=input.numpy(),
            #        output=output.numpy(), target=target.numpy())
    print('FINISHED!')
